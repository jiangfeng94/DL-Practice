## 总结原因
loss =nan 的原因主要有  
1. 训练不收敛
2. 学习率过大
3. 步子太大导致梯度爆炸


## 解决的方式
A.查找问题的原因
弱化场景，简化样本，给模型输入10w个同样的样本；如果还有问题，那么就是网络本身出现了问题；

B.数据归一化（减均值 、 除方差、加入normalization 如BN 或者L2norm）

C.更换数据的初始化方法  如 xavier 或 msra

D.减小学习率，减小batch_size

E.加入gradient clipping  
gradient clipping ：每当梯度到一定的阈值，将他们设置到一个小一些的数字

F:改变网络的宽度
可能原因是后面的层，数据更新异常，增加后面层数的宽度

G:增加层数